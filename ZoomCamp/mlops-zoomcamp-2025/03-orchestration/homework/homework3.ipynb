{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "716e3c5a",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "The goal of this homework is to create a simple training pipeline, use mlflow to track experiments and register best model, but use Mage for it.\n",
    "\n",
    "We'll use [the same NYC taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page), the **Yellow** taxi data for March, 2023. \n",
    "\n",
    "## Question 1. Select the Tool\n",
    "\n",
    "You can use the same tool you used when completing the module,\n",
    "or choose a different one for your homework.\n",
    "\n",
    "What's the name of the orchestrator you chose? \n",
    "\n",
    "**Airflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4235e6",
   "metadata": {},
   "source": [
    "## Question 2. Version\n",
    "\n",
    "What's the version of the orchestrator? \n",
    "\n",
    "**3.0.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b444371",
   "metadata": {},
   "source": [
    "## Question 3. Creating a pipeline\n",
    "\n",
    "Let's read the March 2023 Yellow taxi trips data.\n",
    "\n",
    "How many records did we load? \n",
    "\n",
    "- **3,403,766**\n",
    "\n",
    "(Include a print statement in your code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b5769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame: 3403766\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Downloaded the data from this URL: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_parquet('../data/yellow_tripdata_2023-03.parquet')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()\n",
    "\n",
    "# Count the number of rows in the DataFrame\n",
    "row_count = df.shape[0]\n",
    "# Print the row count\n",
    "print(f\"Number of rows in the DataFrame: {row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a08ce1",
   "metadata": {},
   "source": [
    "## Question 4. Data preparation\n",
    "\n",
    "Let's continue with pipeline creation.\n",
    "\n",
    "We will use the same logic for preparing the data we used previously. \n",
    "\n",
    "This is what we used (adjusted for yellow dataset):\n",
    "\n",
    "```python\n",
    "def read_dataframe(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df.duration = df.duration.dt.total_seconds() / 60\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    return df\n",
    "```\n",
    "\n",
    "Let's apply to the data we loaded in question 3. \n",
    "\n",
    "What's the size of the result? \n",
    "\n",
    "- **3,316,216** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db386d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame: 3316216\n"
     ]
    }
   ],
   "source": [
    "def read_dataframe(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df.duration = df.duration.dt.total_seconds() / 60\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = read_dataframe('../data/yellow_tripdata_2023-03.parquet')\n",
    "# Count the number of rows in the DataFrame\n",
    "row_count = df.shape[0]\n",
    "# Print the row count\n",
    "print(f\"Number of rows in the DataFrame: {row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc2b4fb",
   "metadata": {},
   "source": [
    "## Question 5. Train a model\n",
    "\n",
    "We will now train a linear regression model using the same code as in homework 1.\n",
    "\n",
    "* Fit a dict vectorizer.\n",
    "* Train a linear regression with default parameters.\n",
    "* Use pick up and drop off locations separately, don't create a combination feature.\n",
    "\n",
    "Let's now use it in the pipeline. We will need to create another transformation block, and return both the dict vectorizer and the model.\n",
    "\n",
    "What's the intercept of the model? \n",
    "\n",
    "Hint: print the `intercept_` field in the code block\n",
    "\n",
    "- **24.77**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7403ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (3316216, 518)\n",
      "Dimensionality (number of columns): 518\n",
      "Model intercept: 24.78\n",
      "RMSE on training data: 8.1587\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Feature preparation\n",
    "def prepare_features(df):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Keep only the required columns (PULocationID and DOLocationID)\n",
    "    df_features = df_copy[['PULocationID', 'DOLocationID']]\n",
    "\n",
    "    # Convert dataframe to list of dictionaries with string conversion\n",
    "    dicts = df_features.astype(str).to_dict(orient='records')\n",
    "\n",
    "    # Create and fit a DictVectorizer\n",
    "    dv = DictVectorizer()\n",
    "    X = dv.fit_transform(dicts)\n",
    "\n",
    "    # Get the dimensionality (number of columns)\n",
    "    dimensionality = X.shape[1]\n",
    "\n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    print(f\"Dimensionality (number of columns): {dimensionality}\")\n",
    "\n",
    "    # Features (X) from previous step - the one-hot encoded pickup and dropoff locations\n",
    "    # Target variable (y) - duration\n",
    "    y = df_copy['duration']\n",
    "    \n",
    "    return dv, X, y\n",
    "\n",
    "# Train the model\n",
    "def train_model(X, y):\n",
    "    # Train a linear regression model\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "\n",
    "    # Print the intercept (needed for the question)\n",
    "    print(f\"Model intercept: {lr.intercept_:.2f}\")\n",
    "\n",
    "    # Make predictions on the training data\n",
    "    y_pred = lr.predict(X)\n",
    "\n",
    "    # Calculate RMSE on the training data\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "    print(f\"RMSE on training data: {rmse:.4f}\")\n",
    "    \n",
    "    return lr\n",
    "\n",
    "# Load the prepared data\n",
    "df = read_dataframe('../data/yellow_tripdata_2023-03.parquet')\n",
    "\n",
    "# Prepare features and target\n",
    "dv, X, y = prepare_features(df)\n",
    "\n",
    "# Train the model\n",
    "model = train_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e82a4",
   "metadata": {},
   "source": [
    "## Question 6. Register the model \n",
    "\n",
    "The model is trained, so let's save it with MLFlow.\n",
    "\n",
    "Find the logged model, and find MLModel file. What's the size of the model? (`model_size_bytes` field):\n",
    "\n",
    "* **14,534**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d688ba09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'yellow_taxi_duration_predictor'.\n",
      "Created version '1' of model 'yellow_taxi_duration_predictor'.\n",
      "2025/05/31 14:56:50 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "\u001b[31m2025/05/31 14:56:52 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Run ID: 149a9b28242743a49b75fea8a617fb21\n",
      "Model saved in run 149a9b28242743a49b75fea8a617fb21\n",
      "Model artifacts saved at: /Users/alexandru.huc/Downloads/Development/GitHub/mlops-zoomcamp/03-orchestration/homework/mlruns/0/149a9b28242743a49b75fea8a617fb21/artifacts/model\n",
      "Check the MLmodel file in this directory to find model_size_bytes\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Set MLflow tracking URI - using file system for local storage\n",
    "mlflow.set_tracking_uri(\"./mlruns\")\n",
    "\n",
    "# Start a new MLflow run\n",
    "with mlflow.start_run() as run:\n",
    "    # Log model parameters\n",
    "    mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "    \n",
    "    # Log model metrics (RMSE from previous step)\n",
    "    y_pred = model.predict(X)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    \n",
    "    # Create model signature\n",
    "    signature = infer_signature(X, y_pred)\n",
    "    \n",
    "    # Log the model with additional components (vectorizer)\n",
    "    mlflow_pyfunc_model_path = \"model\"\n",
    "    \n",
    "    # Log the scikit-learn model as an artifact\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path=mlflow_pyfunc_model_path,\n",
    "        signature=signature,\n",
    "        input_example=X[:5],\n",
    "        registered_model_name=\"yellow_taxi_duration_predictor\"\n",
    "    )\n",
    "    \n",
    "    # Log the DictVectorizer as a separate artifact\n",
    "    dict_vectorizer_path = \"dict_vectorizer\"\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=dv,\n",
    "        artifact_path=dict_vectorizer_path\n",
    "    )\n",
    "    \n",
    "    # Print the run ID for reference\n",
    "    print(f\"MLflow Run ID: {run.info.run_id}\")\n",
    "    print(f\"Model saved in run {run.info.run_id}\")\n",
    "    \n",
    "    # Get the path to the MLmodel file\n",
    "    artifacts_uri = mlflow.get_artifact_uri()\n",
    "    model_path = os.path.join(artifacts_uri, mlflow_pyfunc_model_path)\n",
    "    print(f\"Model artifacts saved at: {model_path}\")\n",
    "    print(\"Check the MLmodel file in this directory to find model_size_bytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_venv_py396",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
